{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core library\n",
    "\n",
    "> Helper functions used throughout the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from nbdev.showdoc import *\n",
    "import wget\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def get_dataset(dataset_name: str):\n",
    "    \"\"\"\n",
    "    Download datasets from Google Drive.\n",
    "    \"\"\"\n",
    "\n",
    "    name_to_id = {\n",
    "        \"word2vec-google-news-300.pkl\": \"1dRwSXbFTcQbn8c3V24G92wFY4DXZ1SDt\",\n",
    "        \"imdb.csv\": \"1wF0YEmQOwceJz2d6w4CfhBgydU87dPGl\",\n",
    "        \"housing.csv\": \"1d7oOKdDmZFx8wf0c8OfuTW1FpUyJHABh\",\n",
    "        \"housing_gmaps_data_raw.csv\": \"1R1RUHAXxzrIngRJMFwyp4vZRVICd-I6T\",\n",
    "        \"housing_addresses.csv\": \"1mOK0uyRz5Zs-Qo7mVMlxwtb2xn1E6N9Q\",\n",
    "        \"housing_merged.csv\": \"1bdYuBtIPrKiU-ut2MeSSsL47onPtZrRt\",\n",
    "        \"housing_processed.csv\": \"12PxnWhPg_Pj0yx75vD22gwfdkkx80E6_\"\n",
    "    }\n",
    "\n",
    "    path = '../data/'\n",
    "    gdrive_path = \"https://docs.google.com/uc?export=download&id=\"\n",
    "    if dataset_name in name_to_id:\n",
    "        if os.path.exists(path + dataset_name):\n",
    "            print(f\"Dataset already exists at '{path + dataset_name}' and is not downloaded again.\")\n",
    "            return\n",
    "        try:\n",
    "            file_url =  gdrive_path + name_to_id[dataset_name]\n",
    "            wget.download(file_url, out=path)\n",
    "        except Exception as e:\n",
    "            print(\"Something went wrong during download. Try again.\")\n",
    "            raise e\n",
    "        print(f\"Download of {dataset_name} dataset complete.\")\n",
    "    else:\n",
    "        raise KeyError(\"File not on Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### California Housing Prices\n",
    "This dataset from Kaggle ([link](https://www.kaggle.com/camnugent/california-housing-prices)) is used in the second chapter of Aurélien Géron's recent book *Hands-On Machine learning with Scikit-Learn and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `housing.csv`\n",
    "This dataset pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of housing.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `housing_gmaps_data_raw.csv`\n",
    "This dataset contains the raw outputs of the addresses associated with the coordinates in the `housing.csv` dataset as retrieved with the Google Maps API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of housing_gmaps_data_raw.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('housing_gmaps_data_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `housing_addresses.csv`\n",
    "The `housing_addresses.csv` dataset is a cleaned subset of the `housing_gmaps_data_raw.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of housing_addresses.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('housing_addresses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### housing_merged.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merge of `housing.csv` and `housing_addresses.csv` from lesson 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of housing_merged.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('housing_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### housing_processed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed version of `housing_merged.csv` with no missing values and categorical columns encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of housing_processed.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('housing_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `imdb.csv`\n",
    "The IMDB dataset is available on Kaggle ([link](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)). This is a dataset for binary sentiment classification and provides a set of 25,000 highly polar movie reviews for training and 25,000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of imdb.csv dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('imdb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `word2vec-google-news-300.pkl`\n",
    "\n",
    "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in *Distributed Representations of Words and Phrases and their Compositionality*. This dataset is available from GENSIM ([link](https://github.com/RaRe-Technologies/gensim-data))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download of word2vec-google-news-300.pkl dataset complete.\n"
     ]
    }
   ],
   "source": [
    "get_dataset('word2vec-google-news-300.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def rmse(y, yhat):\n",
    "    \"\"\"A utility function to calculate the Root Mean Square Error (RMSE).\n",
    "    \n",
    "    Args:\n",
    "        y (array): Actual values for target.\n",
    "        yhat (array): Predicted values for target.\n",
    "        \n",
    "    Returns:\n",
    "        rmse (double): The RMSE.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0816659994661326"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([2,2,3])\n",
    "yhat = np.array([0,2,6])\n",
    "rmse(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def print_scores(fitted_model):\n",
    "    res = {\n",
    "        \"RMSE on train:\": rmse(fitted_model.predict(X_train), y_train),\n",
    "        \"R^2 on train:\": fitted_model.score(X_train, y_train),\n",
    "        \"RMSE on valid:\": rmse(fitted_model.predict(X_valid), y_valid),\n",
    "        \"R^2 on valid:\": fitted_model.score(X_valid, y_valid),\n",
    "    }\n",
    "    if hasattr(fitted_model, \"oob_score_\"):\n",
    "        res[\"OOB R^2:\"] = fitted_model.oob_score_\n",
    "\n",
    "    for k, v in res.items():\n",
    "        print(k, round(v, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=True,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train: 1.267\n",
      "R^2 on train: 0.981\n",
      "RMSE on valid: 2.995\n",
      "R^2 on valid: 0.856\n",
      "OOB R^2: 0.862\n"
     ]
    }
   ],
   "source": [
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
