{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 12 - Natural Language Processing I\n",
    "\n",
    "> Analyse text with machine learning - part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "<img src='./images/natural-language-processing-so-hot-right-now.jpg'  width=400>\n",
    "\n",
    "Natural language processing (NLP) is the part of Machine Learning concerned with the analysis of digital, human written texts. The topic of NLP is as old as machine learning itself and dates back to Alan Turing himself. \n",
    "\n",
    "However, there has been exciting and rapid progress in the past couple of years. One of the outstanding achievements was the publication of OpenAI's GPT-2, a language model able to not only create realistic text samples but also solve tasks of many NLP benchmarks without special training. See the figure below for an example output of GPT-2.\n",
    "\n",
    "If you want to try your own examples you can do so at [talktotransformer.com](https://talktotransformer.com/) or read the original article on [OpenAI's webpage](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "<p align=\"left\">\n",
    "<img src='./images/gpt2-example.png' width=400>\n",
    "\n",
    "_Summary:_ In this notebook explore how we can transform text into a format that is machine readable by using **vector encodings**. To get high quality encodings it is often necessary to do some **pre-processing** on the raw texts. This often requires some forms of **string processing** which are briefly explained and showcased. Finally, we use the encodings to build a rudimentary **search enginge**. In summary, this lecture is structured in the following three parts:\n",
    "* Dataset: 20 Newsgroup\n",
    "* String Processing\n",
    "* Vector Encodings\n",
    "* Search Enginge\n",
    "\n",
    "_Created by:_ Leandro von Werra, Spring 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets how many digits of numpy objects are printed \n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this lesson we will use the 20 Newsgroup dataset which is the `Hello World` example in NLP. It contains about 18'000 text snippets that belong to 20 topics. The goal is to assign the texts in the test set to the 20 topics.\n",
    "\n",
    "This dataset can be loaded using the `Scikit-learn` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `newsgroups_train` variable is a dictionary with several keys: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DESCR` contains a text with a description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train['DESCR'][25:394])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` entry contains a list of all texts. We can print the first entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `target` contains the numerical lables wheras the `target_names` contains the text labels as the name suggests. The label that belongs to the example above is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example text this means it belongs to the following category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.hockey'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names'][newsgroups_train['target'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we store the data in a DataFrame, since it is easier to handle the data once it is in that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/leandro/scikit_learn_data/20news_home/2...</td>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/leandro/scikit_learn_data/20news_home/2...</td>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/leandro/scikit_learn_data/20news_home/2...</td>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/leandro/scikit_learn_data/20news_home/2...</td>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/leandro/scikit_learn_data/20news_home/2...</td>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filenames  \\\n",
       "0  /Users/leandro/scikit_learn_data/20news_home/2...   \n",
       "1  /Users/leandro/scikit_learn_data/20news_home/2...   \n",
       "2  /Users/leandro/scikit_learn_data/20news_home/2...   \n",
       "3  /Users/leandro/scikit_learn_data/20news_home/2...   \n",
       "4  /Users/leandro/scikit_learn_data/20news_home/2...   \n",
       "\n",
       "                                                text  target  \n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10  \n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3  \n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17  \n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3  \n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'filenames': newsgroups_train['filenames'],\n",
    "                   'text': newsgroups_train['data'],\n",
    "                   'target': newsgroups_train['target'],})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also do a few necessary dataframe operations:\n",
    "1. Remove the filepath from the filename by taking the piece after last slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filenames'] = df['filenames'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split off additional information from text such as email adresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.split('\\n\\n', 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add the target names to the entries in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_name'] = df['target'].apply(lambda x: newsgroups_train['target_names'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54367</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60215</td>\n",
       "      <td>My brother is in the market for a high-perfo...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76120</td>\n",
       "      <td>\\n\\n\\n|&gt;The student of \"regional killings\" ali...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60771</td>\n",
       "      <td>\\nIn article &lt;1993Apr19.034517.12820@julian.uw...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51882</td>\n",
       "      <td>\\n1)    I have an old Jasmine drive which I c...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filenames                                               text  target  \\\n",
       "0     54367  \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1     60215    My brother is in the market for a high-perfo...       3   \n",
       "2     76120  \\n\\n\\n|>The student of \"regional killings\" ali...      17   \n",
       "3     60771  \\nIn article <1993Apr19.034517.12820@julian.uw...       3   \n",
       "4     51882   \\n1)    I have an old Jasmine drive which I c...       4   \n",
       "\n",
       "                target_name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Create a new column called `text_length` where the length of each text is stored and then  plot a histogram of the text lenghts.\n",
    "* To get the new columne use the `apply` and the function `len` to get the length of the a string. Example usage of `len`:\n",
    "```python\n",
    "len('test')\n",
    ">>> 4\n",
    "```\n",
    "* Then use the `sns.distplot` function to plot the distribution. Set the bins argument of displot to `bins=1000` and then use `plt.xlim([0,1000])`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Processing\n",
    "In this section we have a look at the basics of string processing. Being able to filter/combine/manipulate strings is a crucial skill to do natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string!\n",
      "(But not a very interesting one)\n",
      "\n",
      "\tEnd.\n"
     ]
    }
   ],
   "source": [
    "string = 'This is a string!\\n(But not a very interesting one)\\n\\n\\tEnd.'\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python strings are lists of characters and as such one can iterate through them like lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      "!\n",
      "\n",
      "\n",
      "(\n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "E\n",
      "n\n",
      "d\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for character in string:\n",
    "    print(character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check their length like lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if they contain certain elements like lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'!' in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' not in string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if a **substring** is present in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'very' in string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capitalisation:**\n",
    "There are different ways to manipulate the casing of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'test'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'TEST'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'test'.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding strings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n"
     ]
    }
   ],
   "source": [
    "result = 'a'+'b'\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting strings:**\n",
    "\n",
    "Often we need to split sentences into words or file paths into components. For this task we can use the `split()` function. By default a string is split wherever a whitespace is (this could be normal space, a tab `\\t` or a newline `\\n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'string!',\n",
       " '(But',\n",
       " 'not',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'one)',\n",
       " 'End.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['path', 'to', 'file', 'image.jpg']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'path/to/file/image.jpg'.split('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stripping strings:**\n",
    "\n",
    "Sometimes strings contain leading or trailing characters that we want to get rid of, such as whitespaces or unnecessary characters. We can remove them with the `strip()` function. Like the `split()` function it removes whitespaces by default but we can set any characters we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'path/to/file/image.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_path/to/file/image.jpg_'.strip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'path/to/file/image.jpg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_-_path/to/file/image.jpg,_,'.strip(',_-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing:**\n",
    "\n",
    "With the `replace()` function one can replace substrings in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one plus one equals three!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'one plus one equals two!'.replace('two','three')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining strings**\n",
    "\n",
    "Sometimes we split strings into a list of words for processing (like stemming or stop word removal) and then want to join them back to a single string. To to this we can use the `join()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a list of words'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['this', 'is', 'a', 'list', 'of', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this-is-a-list-of-words'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'.join(['this', 'is', 'a', 'list', 'of', 'words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Write a function that performs the following on `string_1` \n",
    "- split the string into words with spaces\n",
    "- then strip the special character `/` from each word\n",
    "- join the words back together with single spaces (`' '`)\n",
    "- make the whole string lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string!\n",
      "/(But not a very interesting one)/\n",
      "\n",
      "\tEnd.\n"
     ]
    }
   ],
   "source": [
    "string_1 = 'This is a string!\\n/(But not a very interesting one)/\\n\\n\\tEnd.'\n",
    "print(string_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are armed with this arsenal of string processing tools, we can pre-process the texts in the dataset to bring them to a cleaner form.\n",
    "\n",
    "One of the richest Python libraries to process texts is the Natural Language Toolkit (NLTK). To install it run the follwing command in your environment:\n",
    "```bash\n",
    "> pip install nltk\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the following steps to clean-up the texts:\n",
    "* Normalization\n",
    "* Tokenization\n",
    "* Remove Stop-Words\n",
    "* Remove Non-Alphabetical Tokens\n",
    "* Stemming\n",
    "\n",
    "We'll do this on one text as an example and then build a function and apply it to all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = df.loc[0, 'text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "This is the process of transforming the text to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "i am sure some bashers of pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent pens massacre of the devils. actually,\n",
      "i am  bit puzzled too and a bit relieved. however, i am going to put an end\n",
      "to non-pittsburghers' relief with a bit of praise for the pens. man, they\n",
      "are killing those devils worse than i thought. jagr just showed you why\n",
      "he is much better than his regular season stats. he is also a lot\n",
      "fo fun to watch in the playoffs. bowman should let jagr have a lot of\n",
      "fun in the next couple of games since the pens are going to beat the pulp out of jersey anyway. i was very disappointed not to see the islanders lose the final\n",
      "regular season game.          pens rule!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Now we split the text in words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'pens', 'massacre', 'of', 'the', 'devils', '.', 'actually', ',', 'i', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.', 'however', ',', 'i', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-pittsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pens', '.', 'man', ',', 'they', 'are', 'killing', 'those', 'devils', 'worse', 'than', 'i', 'thought', '.', 'jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.', 'he', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', '.', 'i', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.', 'pens', 'rule', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Next, we remove words that are too common and don't add the the content of sentences. These words are commonly called 'stop words'. NLTK provides a list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'will', 'itself', 'them', 'all', 'aren', \"weren't\", 'than', 'this', 'couldn', \"shan't\", \"you'd\", 'needn', 'wasn', 'myself', 'again', 'below', 'has', 'at', 'mightn', \"it's\", \"needn't\", \"should've\", 'while', \"mustn't\", 'ain', 'weren', 'shouldn', 'out', 'doesn', 'won', \"hadn't\", 'hadn', 'you', 'as', 'nor', 'so', 'for', 'once', 'such', 'they', 'my', 'being', \"you'll\", 'with', 'who', 'very', 'few', 'haven', \"won't\", 've', 'm', 'of', 'where', 'more', 'down', 'only', 'off', \"mightn't\", 'in', 'above', 'about', 'll', 'be', 'if', 'over', 'through', 'here', \"doesn't\", 'each', 'mustn', \"hasn't\", \"you've\", 'don', 'd', 'what', 'am', 'is', 'him', 'that', 'why', 'their', 'should', 'do', 'his', 'she', 'other', 'doing', \"you're\", 'from', 'under', \"wouldn't\", 'an', 'herself', 'most', 'a', \"wasn't\", 'her', 'we', 'ma', 'its', 'before', 'were', 'too', 'o', 'shan', 'wouldn', 'because', 'further', 'by', 'yours', 'after', 'some', 'been', 's', 'to', 'hasn', 'the', 'then', 'theirs', 't', 'just', 'those', \"aren't\", 'it', 'had', 'on', \"that'll\", \"didn't\", 'and', 'hers', 'up', 'there', 'ours', \"don't\", 'yourself', 'was', 'or', 'same', 'didn', 'between', 'into', 'your', 'yourselves', 're', 'y', 'whom', \"couldn't\", 'himself', 'have', 'did', \"isn't\", \"haven't\", 'ourselves', 'until', 'now', 'themselves', 'how', \"shouldn't\", 'i', 'when', 'both', 'can', 'any', 'but', 'during', 'he', \"she's\", 'these', 'having', 'not', 'does', 'our', 'own', 'are', 'no', 'which', 'against', 'me', 'isn'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only the words that are **not** in the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', '.', 'actually', ',', 'bit', 'puzzled', 'bit', 'relieved', '.', 'however', ',', 'going', 'put', 'end', 'non-pittsburghers', \"'\", 'relief', 'bit', 'praise', 'pens', '.', 'man', ',', 'killing', 'devils', 'worse', 'thought', '.', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', '.', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', '.', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', '.', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', '.', 'pens', 'rule', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = [i for i in tokens if not i in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "We also want to get of all tokens that are not composed of letters (e.g. punctuation and numbers). We can check if a words is only composed of alphabetic letters with the `isalpha()` and filter with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'relief', 'bit', 'praise', 'pens', 'man', 'killing', 'devils', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n"
     ]
    }
   ],
   "source": [
    "tokens = [i for i in tokens if i.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "As a final step we want to trim the words to the stem. This helps drastically decrease the vocabulary size and maps similar/same words onto the same word. E.g. plural/singular words or different forms of verbs:\n",
    "* pen, pens --> pen\n",
    "* happy, happier --> happi\n",
    "* go, goes --> go\n",
    "\n",
    "There are several languages available in nltk since this is a **language dependant process**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied to the text sample this yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil', 'actual', 'bit', 'puzzl', 'bit', 'reliev', 'howev', 'go', 'put', 'end', 'relief', 'bit', 'prais', 'pen', 'man', 'kill', 'devil', 'wors', 'thought', 'jagr', 'show', 'much', 'better', 'regular', 'season', 'stat', 'also', 'lot', 'fo', 'fun', 'watch', 'playoff', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'coupl', 'game', 'sinc', 'pen', 'go', 'beat', 'pulp', 'jersey', 'anyway', 'disappoint', 'see', 'island', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokens = [stemmer.stem(i) for i in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Put all preprocessing steps into a function `preprocessing(text)` that returns a list of clean tokens like above and apply it to all texts creating a new column `processed_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this function to the whole corpus takes a while (ca. 1min 30s on my machine). Sometimes it is handy to have a progress bar to see how well/fast we are doing. There is a cool library called `tqdm` which provides easy to use progress bars.\n",
    "\n",
    "If you want to experiment with it, you can install it with:\n",
    "```bash\n",
    "> pip install tqdm\n",
    "```\n",
    "\n",
    "After importing you need to register it with pandas with `tqdm.pandas()`. If you then use `progress_apply` instead of `apply` you get a progress bar during the computation showing you the progress. If you have problems installing it or don't want to use it, just use the normal `apply` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 18846/18846 [01:29<00:00, 209.89it/s]\n"
     ]
    }
   ],
   "source": [
    "df['processed_text'] = df['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54367</td>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>[sure, basher, pen, fan, pretti, confus, lack,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60215</td>\n",
       "      <td>My brother is in the market for a high-perfo...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>[brother, market, video, card, support, vesa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76120</td>\n",
       "      <td>\\n\\n\\n|&gt;The student of \"regional killings\" ali...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[student, region, kill, alia, davidian, davidi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60771</td>\n",
       "      <td>\\nIn article &lt;1993Apr19.034517.12820@julian.uw...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>[articl, wlsmith, wayn, smith, write, articl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51882</td>\n",
       "      <td>\\n1)    I have an old Jasmine drive which I c...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>[old, jasmin, drive, use, new, system, underst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filenames                                               text  target  \\\n",
       "0     54367  \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1     60215    My brother is in the market for a high-perfo...       3   \n",
       "2     76120  \\n\\n\\n|>The student of \"regional killings\" ali...      17   \n",
       "3     60771  \\nIn article <1993Apr19.034517.12820@julian.uw...       3   \n",
       "4     51882   \\n1)    I have an old Jasmine drive which I c...       4   \n",
       "\n",
       "                target_name                                     processed_text  \n",
       "0          rec.sport.hockey  [sure, basher, pen, fan, pretti, confus, lack,...  \n",
       "1  comp.sys.ibm.pc.hardware  [brother, market, video, card, support, vesa, ...  \n",
       "2     talk.politics.mideast  [student, region, kill, alia, davidian, davidi...  \n",
       "3  comp.sys.ibm.pc.hardware  [articl, wlsmith, wayn, smith, write, articl, ...  \n",
       "4     comp.sys.mac.hardware  [old, jasmin, drive, use, new, system, underst...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encodings\n",
    "Now that we cleaned up and tokenized the text corpus we are now ready to encode the texts in vectors. In class we had a look at simple **one-hot encodings** that can be extended to count encodings and **TF-IDF encodings**.\n",
    "\n",
    "Scikit-learn comes with functions to do both count and TF-IDF encodings on text. The interface is very similar to the classifier just the `predict` step is replace with `transform`:\n",
    "\n",
    "```python\n",
    "count_vectorizer = CountVectorizer(your_settings)\n",
    "count_vectorizer.fit(your_dataset)\n",
    "vec = count_vectorizer.transform('your_text')\n",
    "```\n",
    "\n",
    "This creates a vectorizer that can transform texts to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn vectorizers come with a set of rudimentary preprocessors and tokenizers by default, but since we have already done these steps we replace the default values with the identity function does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also limit the number of words take into account when building the vector. This limits the vector size and cuts off words that occur rarely.\n",
    "\n",
    "**Example:** If you set `vocab_size=10000` only the 10000 most occurring words are used to build the vector and all rare words are excluded. This means that the encoding vector then has a dimension of 10000. \n",
    "\n",
    "For now we take all words (`vocab_size=None`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer='word', tokenizer=identity, preprocessor=identity, token_pattern=None, max_features=vocab_size)\n",
    "count_vec = CountVectorizer(analyzer='word', tokenizer=identity, preprocessor=identity, token_pattern=None, max_features=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test both vectorizers on a small, dummy dataset with **4 documents**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['this','is','the','first','document','in','the','corpus'],\n",
    "    ['this','document','is','the','second','document','in','the','corpus'],\n",
    "    ['and','this','is','the','third','one','in','this','corpus'],\n",
    "    ['is','this','the','first','document','in','this','corpus'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a count vectorizer to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1),\n",
       "        preprocessor=<function identity at 0x116c73400>, stop_words=None,\n",
       "        strip_accents=None, token_pattern=None,\n",
       "        tokenizer=<function identity at 0x116c73400>, vocabulary=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the a vectorizer is fitted, we can investigate the vocabulary. It is a dictionary that points each word to the index in the vector it corresponds to. For example the word `'this'` corresponds to the 10+1 (+1 because we start counting at zero) entry in the vector and the word `'and'` corresponds to the the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 10,\n",
       " 'is': 5,\n",
       " 'the': 8,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'in': 4,\n",
       " 'corpus': 1,\n",
       " 'second': 7,\n",
       " 'and': 0,\n",
       " 'third': 9,\n",
       " 'one': 6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform the corpus and get a list of vectors in the form of a matrix (each row corresponds to a document vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 0 0 2 0 1]\n",
      " [0 1 2 0 1 1 0 1 2 0 1]\n",
      " [1 1 0 0 1 1 1 0 1 1 2]\n",
      " [0 1 1 1 1 1 0 0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "X = count_vec.transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now do the same thing with the TF-IDF vectorizer we see that the output looks different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.291 0.356 0.44  0.291 0.291 0.    0.    0.583 0.    0.291]\n",
      " [0.    0.238 0.582 0.    0.238 0.238 0.    0.456 0.476 0.    0.238]\n",
      " [0.439 0.229 0.    0.    0.229 0.229 0.439 0.    0.229 0.439 0.459]\n",
      " [0.    0.291 0.356 0.44  0.291 0.291 0.    0.    0.291 0.    0.583]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec.fit(corpus)\n",
    "X = tfidf_vec.transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The shape of the matrix is the same.\n",
    "* Instead of integers (corresponding to counts) we have continous values.\n",
    "* Elements that occur in multilple documents have lower scores than those appearing in fewer.\n",
    "\n",
    "This should just illustrate how count and TF-IDF vectorizer work. Now let's apply this to our dataset and create encodigs with `50000` words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50000\n",
    "tfidf_vec = TfidfVectorizer(analyzer='word', tokenizer=identity, preprocessor=identity, token_pattern=None, max_features=vocab_size)\n",
    "count_vec = CountVectorizer(analyzer='word', tokenizer=identity, preprocessor=identity, token_pattern=None, max_features=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we used the `fit` and `transform` function. We can avoid these two steps with the combined function `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vec.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count = count_vec.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields a vocabulary with `50000` entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of the returned matrix we see that it still has as many rows as the input but now has `50000` entries per row (the feature vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 50000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the texts were converted into vectors of size 67390. This is the number of unique words in the dataset. This feature space is **significantly larger** than what we saw so far: 67'390 vs. ~10-20 in the titanic dataset. This is one challenging aspect of NLP: very large, yet sparse (most of the entries are zero) input matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these encodings to build ourselves a rudimentary **search engine**. We will see that the TF-IDF yield much better results than count vectors.\n",
    "\n",
    "In information retrieval jargon a question or term that is searched in a corpus of documents is called a `query`. Let define a example query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'i want to buy a mustang'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the query with both the count and TF-IDF vectorizer. Note that we duplicate the vector `n_documents` times. This is not really necessary, but makes the comparison of the query vector with the documents easier since they then have both the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query tokens: ['want', 'buy', 'mustang']\n",
      "query and corpus shapes: (18846, 50000) (18846, 50000)\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocessing(query)\n",
    "print('query tokens:', tokens)\n",
    "n_documents = np.shape(X_count)[0]\n",
    "\n",
    "query_vec_count = count_vec.transform([tokens]*n_documents)\n",
    "query_vec_tfidf = tfidf_vec.transform([tokens]*n_documents)\n",
    "\n",
    "print('query and corpus shapes:', np.shape(query_vec_tfidf), np.shape(X_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the encodings we use the `cosine_similarity`, since it is best suited for high-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we usse this to calculate a similarity between each document and the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_count = cosine_similarity(X_count, query_vec_count)[:, 0]\n",
    "sim_tfidf = cosine_similarity(X_tfidf, query_vec_tfidf)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost done, we only need to find the of the similarity array with the highest similarity with `np.argmax` and display the `text` in the dataframe at this position. We can display the best search result for both encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm looking to buy a '92 Toyota Previa All-Trac with low miles.\n",
      "If you are selling one, or want someone to buy out an existing lease,\n",
      "please contact me by mail.\n",
      "\n",
      "-- \n",
      "Will Estes\t\tInternet: westes@netcom.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[np.argmax(sim_count), 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petebre@elof.iit.edu (BrentA. Peterson) writes:\n",
      ">jmh@hopper.Virginia.EDU (Jeffrey Hoffmeister) writes:\n",
      ">>jmm4h@Virginia.EDU (\"The Bald Runner\") writes:\n",
      "\n",
      ">>>I just have got to remind all of you that this is it!  Yes,\n",
      ">>>that's right, somtime this fall, Ford (the granddaddy of cars)\n",
      ">>>will be introducing an all-new, mega-cool\n",
      ">>>way-too-fast-for-Accord-drivers Mustang.  It's supposed to be\n",
      ">>>100% streamlined, looking similar to the Mach III concept car\n",
      ">>>Ford came out with around January.  I can't wait.  Anyone out\n",
      ">>>there hear anything about it recently?\n",
      "\n",
      ">>If everything I've read is correct, Ford is doing nothing but \"re-\n",
      ">>skinning\" the existing Mustang, with MINOR suspension modifications.\n",
      ">>And the pictures I've seen indicate they didn't do a very good job\n",
      ">>of it.\n",
      ">>The \"new\" mustang, is nothing but a re-cycle of a 20 year old car.\n",
      "\n",
      ">gee.... is it 1999 already?\n",
      ">Yes, it will still be on the fox program chasis, anything that will be differe\n",
      ">nt on the new car as far as mechanical's is unknown. The suspension will most\n",
      ">likely be changed, as well as the drive drain. From what has been printed on\n",
      ">it, there is no clear idea of what will be done, as some say it will have\n",
      ">the modular V8 and others the current small block... just have to wait and see\n",
      ">Also is far as styling goes from what I seen is good, a return to tradition.\n",
      ">C scoop on the sides and roof line much like a '65 or '66 fastback.\n",
      "\n",
      "You know, I'm a Ford fan, I must say, so I'm looking forward to the next\n",
      "Mustang.  I have faith that it will be a fine product, more desireable\n",
      "than the Camaro is now.  You know, that's MHO.  \n",
      "\n",
      "The differences these days between Ford and GM are not so much the quality,\n",
      "just the philosophy.  It used to be quality _and_ philosophy.  GM is\n",
      "barely catching up, but they have more room for improvement that\n",
      "can only be made up in time.  STSs still come off the assembly line\n",
      "with screwed up paint stripes and poor trunk/door/hood/panel alignments;\n",
      "it's those 75 year old plants.  And the latest GM products still come\n",
      "with the standard equipment RattleDash (tm).  But like I said, they're\n",
      "getting better and making the move in the right direction.\n",
      "\n",
      "They beat Ford to the market with the Camaro/Firebird, but really only\n",
      "in words.  Production of these vehicles will be limited until the\n",
      "end of the year, keeping selling prices above MSRP for the most part\n",
      "since there are so many twitching Camaro fans out there.  I wouldn't\n",
      "press Ford to hurry the Mustang since the final wait could be worth it.\n",
      "Besides, no bow-tie fanatic is gonna buy the Mustang anyway.\n",
      "\n",
      "I do not put much stock in the mag rags' \"inside\" information, or even\n",
      "Ford rep quotes.  The Taurus was pretty much a surprise when it was\n",
      "finally disclosed in it's entirety.  \"Inside\" information had the\n",
      "Taurus with a V8 and rear-wheel drive at one point.  I wouldn't look\n",
      "for a simple re-paneled Mustang, folks; you may be cheating yourself\n",
      "if you do.  There's a lot of potential.  Ford hasn't released a new\n",
      "car without a 4-wheel IS in 7 years.  The Mustang project has been\n",
      "brewing for at least 4, right?  A 4-wheel IS could happen.  Those\n",
      "modular V8's are out there, too.  In the interest of CAFE and\n",
      "competition, don't rule those out, either.   Your ignorant if you do.\n",
      "And there are so many spy shots and artist renderings out there,\n",
      "who really knows what it'll look like?  The Mach III?  Doubt it.\n",
      "Highly.\n",
      "\n",
      "The next Mustang will be Ford's highest profile car.  It attracts\n",
      "way more attention than the Camaro/Firebird because it's heritage\n",
      "is more embedded in the general public.  Don't lie to yourself and\n",
      "believe Ford will forfeit that.\n",
      "\n",
      "I submit that the Mustang will be a success.  Enough to elicit\n",
      "defensive remarks from some heavy Camaro fans here.  You know,\n",
      "intelligent, critical spews like, \"The Mustang bites, man!\"  Some of\n",
      "you are already beginning.  I predict that the Mustang and Camaro\n",
      "will be comparable performers, as usual.  I predict that the\n",
      "differences will be in subjective areas like looks and feel, as usual.\n",
      "The Camaro is still a huge automobile; the Mustang will retain its\n",
      "cab-rearward styling and short, pony-car wheelbase.  The Camaro still\n",
      "reaches out to the fighter pilot, while the Mustang will appeal to\n",
      "the driver.  The Camaro will still sell to the muscle car set, while\n",
      "the Mustang will continue to sell to the college-degreed muscle car set.\n",
      "Both will be more refined (I do think the Camaro is).  There will be\n",
      "no clear winner.\n",
      "\n",
      "Unless the Ford gets the 32v, 300hp Romeo.  You don't seriously believe\n",
      "that it was designed for the Mark VIII only, do you?\n",
      "\n",
      ":^)\n",
      "\n",
      "Regards,\n",
      "\n",
      "Brian\n",
      "\n",
      "bqueiser@magnus.acs.ohio-state.edu\n",
      "------------------------------------------------------------------------\n",
      "I am the engineer, I can choose K.\n",
      "------------------------------------------------------------------------\n",
      "Department of Engineering Mechanics\n",
      "Ohio State University\n",
      "Columbus, OH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[np.argmax(sim_tfidf), 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Can you explain why the count vectorizer chose the first text although it doesn't say anything about `'mustang'`? Why are longer documents generally penalized with the cosine-similarity score (_hint:_ look at the definition of cosine similarity)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
